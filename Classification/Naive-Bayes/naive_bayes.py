# -*- coding: utf-8 -*-
"""Naive-bayes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18g82U6FYjvKUgTT13o43zOFM43-xiPRp

**Title: Pima Indians Diabetes Database**
"""

# Importing the libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ML libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score

# loading data

df = pd.read_csv("naive-bayesdiabetes.csv")

df.head()

# Basic EDA

df.info()
df.describe()
df.isnull().sum()

# checking class balance

sns.countplot(x="Outcome", data=df)
plt.title("Class Distribution")
plt.show()

# Data Cleaning
# Replacing 0 with median

cols_with_zero = ["Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI"]

for col in cols_with_zero:
    df[col] = df[col].replace(0, np.nan)
    df[col].fillna(df[col].median(), inplace=True)

df.isnull().sum()

# spliting input and output

X = df.drop("Outcome", axis=1)
y = df["Outcome"]

# Train test split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)



# Feature Scaling
# Important for Gaussian NB.

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# choosing algorithm and training the model

from sklearn.naive_bayes import GaussianNB

nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

y_pred_nb = nb_model.predict(X_test)

# Evaluation

print("Naive Bayes Accuracy:", accuracy_score(y_test, y_pred_nb))

# confusion matrix

cm = confusion_matrix(y_test, y_pred_nb)

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Naive Bayes")
plt.show()

# Accuracy is 75% so later on we required to do hypertuining

# Take input from user
preg = int(input("Enter Pregnancies: "))
glu = float(input("Enter Glucose: "))
bp = float(input("Enter Blood Pressure: "))
skin = float(input("Enter Skin Thickness: "))
ins = float(input("Enter Insulin: "))
bmi = float(input("Enter BMI: "))
dpf = float(input("Enter Diabetes Pedigree Function: "))
age = int(input("Enter Age: "))

# Convert into DataFrame
import pandas as pd

new_data = pd.DataFrame([[preg, glu, bp, skin, ins, bmi, dpf, age]],
                        columns=["Pregnancies", "Glucose", "BloodPressure",
                                 "SkinThickness", "Insulin", "BMI",
                                 "DiabetesPedigreeFunction", "Age"])

# Apply scaling
new_data_scaled = scaler.transform(new_data)

# Predict
prediction = nb_model.predict(new_data_scaled)
probability = nb_model.predict_proba(new_data_scaled)

# Output result
if prediction[0] == 1:
    print("\nResult: Patient is likely to have Diabetes")
else:
    print("\nResult: Patient is unlikely to have Diabetes")

print("Probability of Diabetes:", probability[0][1])

# Accuracy is 75% so later on we required to do hypertuining