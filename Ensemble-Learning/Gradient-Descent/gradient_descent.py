# -*- coding: utf-8 -*-
"""Gradient-Descent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12m8-bPsgK0ZYrZ6HFC0MTf8wF7CS8kVD

**Title: Performing Gradient Descent of Pima Dataset**
"""

# impoting libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('pima-dataset-Gradient-descent.csv')

df.head()

df.info()

df.describe()

# Handle Zero Values (Important for Pima)

cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

for col in cols:
    df[col] = df[col].replace(0, df[col].median())

# Splitting feature & Target

X = df.drop('Outcome', axis=1).values
y = df['Outcome'].values.reshape(-1,1)

# Train test split

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Feature scaling , choosing model & Training

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Add bias term (intercept)

X_train = np.c_[np.ones((X_train.shape[0], 1)), X_train]
X_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]

# We add 1’s so the model can learn the intercept (starting point) properly.
# Without bias → line must start from 0
# With bias → line can move up or down

# Define sigmoid function

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Why do we need sigmoid?
# Because we are doing classification (0 or 1).

# Define cost function - Define Cost Function

# If mistake is big → we adjust weights more.
# If mistake is small → we are close to correct.
# This value is called cost (or error).

def compute_cost(X, y, weights): #
    m = len(y)
    predictions = sigmoid(np.dot(X, weights))
    cost = -(1/m) * np.sum(
        y*np.log(predictions) + (1-y)*np.log(1-predictions)
    )
    return cost

# Gradient descent implementation

def gradient_descent(X, y, weights, learning_rate, iterations):
    m = len(y)
    cost_history = []

    for i in range(iterations):
        predictions = sigmoid(np.dot(X, weights))

        gradient = (1/m) * np.dot(X.T, (predictions - y))

        weights = weights - learning_rate * gradient

        cost = compute_cost(X, y, weights)
        cost_history.append(cost)

    return weights, cost_history

# Inatilize weights

weights = np.zeros((X_train.shape[1], 1))
learning_rate = 0.01
iterations = 1000

# Train model

weights, cost_history = gradient_descent(
    X_train, y_train, weights, learning_rate, iterations
)

# PLot cost decresing

plt.plot(cost_history)
plt.xlabel("Iterations")
plt.ylabel("Cost")
plt.title("Cost Reduction Over Iterations")
plt.show()

# Make prediction

def predict(X, weights):
    probabilities = sigmoid(np.dot(X, weights))
    return [1 if i >= 0.5 else 0 for i in probabilities]

y_pred = predict(X_test, weights)

# Calculate accuracy
y_test_flat = y_test.flatten()

accuracy = np.mean(y_pred == y_test_flat) * 100
print("Accuracy:", accuracy)

