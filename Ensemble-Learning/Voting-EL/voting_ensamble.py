# -*- coding: utf-8 -*-
"""Voting-ensamble.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tiQgOKUJJmApnfGL0GnvhpgbG6hsBEFz

**Voting: Ensemble learning**

Ensemble learning mean we are combining multiple models together to make stronger model.

Voting means combining prediction of multiple models and choosing final answer based on majority

Example:

Logistic Regression: 1

Decision Tree: 0

SVM: 1

Majority 1, so final output is 1.

There are two types
1. Hard

2. Soft

Hard: model gives class label

Soft : Instead of voting on final label , we average probability
"""

# Importing libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('pima-diabeties-voting-ensamble.csv')

df.head()

df.info()

# Basic EDA

print(df.info())
print(df.describe())
print(df.isnull().sum())

# Data cleaning
# In Pima dataset, some columns contain 0 which is not valid, Replace 0 with median:

plt.figure(figsize=(12,6))
sns.boxplot(data=df)
plt.xticks(rotation=45)
plt.title("Boxplot of All Features")
plt.show()

df['Outcome'].unique()

# code for getting zeros in column

(df == 0).sum()

# so here we got the zero entries so we have to replaced with mean or median according to data

# For replacing the value we have to make use of replace

df['Pregnancies']= df['Pregnancies'].replace(0, df['Pregnancies'].mean())

# Apply Median (for columns with outliers)
df['BloodPressure'] = df['BloodPressure'].replace(0, df['BloodPressure'].median())
df['BMI'] = df['BMI'].replace(0, df['BMI'].median())
df['Age'] = df['Age'].replace(0, df['Age'].median())

# Apply Mean (for columns without outliers)
df['Glucose'] = df['Glucose'].replace(0, df['Glucose'].mean())
df['Glucose'] = df['Glucose'].replace(0, df['Glucose'].mean())
df['SkinThickness'] = df['SkinThickness'].replace(0, df['SkinThickness'].mean())
df['Insulin'] = df['Insulin'].replace(0, df['Insulin'].mean())
df['DiabetesPedigreeFunction'] = df['DiabetesPedigreeFunction'].replace(0, df['DiabetesPedigreeFunction'].mean())

(df == 0).sum()

(df == 0).sum()

# Visualizatoon of output column
# we have one fucntion with the name of count plot that is used to get the distribution

sns.countplot(x='Outcome', data=df)
plt.title("Diabetes Distribution")
plt.show()

# Output distribution in numbers

df['Outcome'].value_counts()

# So here the distribution is fine like its 65 and 35
# we have to make chage when we do have distribution like 95% - 5%

# splitting data into input and output columns

X = df.drop("Outcome", axis=1)
y = df['Outcome']

X

y

X.shape

y.shape

# Splitting the data into training and testing part

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_train = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

X_train.shape

X_test.shape

y_train.shape

y_train.shape

# feature Scaling & choosing model and training the model

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# choosing multiple algorithm for voting

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

# checking bias and variance

# code to use when we use only single model

# from sklearn.metrics import accuracy_score
# train_accuracy = model.score(X_train, y_train)
# test_accuracy = model.score(X_test, y_test)

# print("Train Accuracy:", train_accuracy)
# print("Test Accuracy:", test_accuracy)

# choosing multiple algorithm for voting

models = [
    LogisticRegression(max_iter=1000),
    DecisionTreeClassifier(random_state=42),
    SVC(probability=True, random_state=42)
]

for model in models:
    model.fit(X_train, y_train)
    print(model.__class__.__name__,
          "Train Accuracy:", model.score(X_train, y_train),
          "Test Accuracy:", model.score(X_test, y_test))

# if getting error then use below code

# ✅ FIX: Make sure X and y are split together (MOST IMPORTANT)

from sklearn.model_selection import train_test_split

X = df.drop('Outcome', axis=1)
y = df['Outcome']

# Split ONCE properly
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Scaling AFTER split
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

models = [
    LogisticRegression(max_iter=1000),
    DecisionTreeClassifier(random_state=42),
    SVC(probability=True, random_state=42)
]

# Train & Evaluate
for model in models:
    model.fit(X_train, y_train)
    print(model.__class__.__name__,
          "Train Accuracy:", model.score(X_train, y_train),
          "Test Accuracy:", model.score(X_test, y_test))

#
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# hard voting

voting_hard = VotingClassifier(
    estimators=[
        ('lr', log_model),
        ('dt', tree_model),
        ('svm', svm_model)
    ],
    voting='hard'
)

voting_hard.fit(X_train, y_train)

y_pred_hard = voting_hard.predict(X_test)

print("Hard Voting Accuracy:",
      accuracy_score(y_test, y_pred_hard))

# soft voting

voting_soft = VotingClassifier(
    estimators=[
        ('lr', log_model),
        ('dt', tree_model),
        ('svm', svm_model)
    ],
    voting='soft'
)

voting_soft.fit(X_train, y_train)

y_pred_soft = voting_soft.predict(X_test)

print("Soft Voting Accuracy:",
      accuracy_score(y_test, y_pred_soft))



# we will be using cofusion matrix when we will bw learning cross validation we will fix using with the help of cross validation

# Output for new data

import numpy as np

# ---- Take input from user ----
preg = int(input("Enter Pregnancies: "))
glu = float(input("Enter Glucose: "))
bp = float(input("Enter Blood Pressure: "))
skin = float(input("Enter Skin Thickness: "))
ins = float(input("Enter Insulin: "))
bmi = float(input("Enter BMI: "))
dpf = float(input("Enter Diabetes Pedigree Function: "))
age = int(input("Enter Age: "))

# ---- Convert to array ----
new_data = np.array([[preg, glu, bp, skin, ins, bmi, dpf, age]])

# ---- Apply same scaling ----
new_data_scaled = scaler.transform(new_data)

# ---- Predict (use whichever model you want) ----
prediction = log_model.predict(new_data_scaled)

# ---- Output ----
if prediction[0] == 1:
    print("Person has Diabetes")
else:
    print("Person does NOT have Diabetes")

# Model must first learn from data using .fit()
# Only after that it can predict using .predict()

log_model.fit(X_train, y_train)

# 1️⃣ Train model
log_model.fit(X_train, y_train)

# 2️⃣ Then predict
prediction = log_model.predict(new_data_scaled)

import numpy as np

# ---- Take input from user ----
preg = int(input("Enter Pregnancies: "))
glu = float(input("Enter Glucose: "))
bp = float(input("Enter Blood Pressure: "))
skin = float(input("Enter Skin Thickness: "))
ins = float(input("Enter Insulin: "))
bmi = float(input("Enter BMI: "))
dpf = float(input("Enter Diabetes Pedigree Function: "))
age = int(input("Enter Age: "))

# ---- Convert to array ----
new_data = np.array([[preg, glu, bp, skin, ins, bmi, dpf, age]])

# ---- Apply same scaling ----
new_data_scaled = scaler.transform(new_data)

# ---- Predict (use whichever model you want) ----
prediction = log_model.predict(new_data_scaled)

# ---- Output ----
if prediction[0] == 1:
    print("Person has Diabetes")
else:
    print("Person does NOT have Diabetes")

