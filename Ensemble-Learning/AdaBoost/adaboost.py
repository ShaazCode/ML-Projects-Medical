# -*- coding: utf-8 -*-
"""Adaboost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zC9g-3uTLw7qIuTiPR576xJWsWvBXBLZ

**Title: AdaBoost on Pima dataset**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('pima-diabeteis-Adaboost.csv')

df.head()

# EDA

df.info()

df.describe()

# EDA - checking outcome distribution

sns.countplot(x='Outcome', data=df)
plt.title("Target Distribution")
plt.show()

# Replacing zeros values

cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

for col in cols:
    df[col] = df[col].replace(0, df[col].median())

# Splitting into X & Y

X = df.drop('Outcome', axis=1)
y = df['Outcome']

# Train test split

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

X_train.shape
# 80% for trainign

X.shape

y_train.shape

y_train.shape

# choosing model, traingnin, and Standarization

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Building AdaBoost model

from sklearn.ensemble import AdaBoostClassifier

from sklearn.tree import DecisionTreeClassifier

base_model = DecisionTreeClassifier(max_depth=1) # we are choosing decision tree model
# base_model = DecisionTreeClassifier(max_depth=1)
# max_depth=1 â†’ Tree can split only once.
# why 1 because Adaboost work with weak learners
ada_model = AdaBoostClassifier(
    estimator=base_model, # Use this base_model (decision stump) as the weak learner."
    n_estimators=50, # Create 50 weak models.
    # So ada Boost will train 50 tree. tree 1 tree 2, tree 3.... tree 50
    learning_rate=1, # How much each tree contributes to final prediction.
    random_state=42 # every time you the code you get the same output
)

ada_model.fit(X_train, y_train) # finally train the model

base_model = DecisionTreeClassifier(max_depth=1)

ada_model = AdaBoostClassifier(
    estimator = base_model,
    n_estimators = 50,
    learning_rate = 1,
    random_state = 42
)

# Prediction

y_pred = ada_model.predict(X_test)

ada_model.fit(X_train, y_train)
y_pred = ada_model.predict(X_test)

# Evaluation

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("Accuracy:", accuracy_score(y_test, y_pred))

# confusion matrix

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# classficaiton report

print(classification_report(y_test, y_pred))

# check train test accuracy

train_acc = ada_model.score(X_train, y_train)
test_acc = ada_model.score(X_test, y_test)

print("Train Accuracy:", train_acc)
print("Test Accuracy:", test_acc)

# Prediction for new values

preg = 2
glu = 120
bp = 70
skin = 25
ins = 100
bmi = 28.5
dpf = 0.45
age = 35

new_data = np.array([[preg, glu, bp, skin, ins, bmi, dpf, age]])
new_data_scaled = scaler.transform(new_data)

prediction = ada_model.predict(new_data_scaled)

if prediction[0] == 1:
    print("Person has Diabetes")
else:
    print("Person does NOT have Diabetes")

# so here we got the accuracy of 75%

base_model = DecisionTreeClassifier(max_depth=1)

ada_base = AdaBoostClassifier(
    estimator = base_model,
    n_estimators= 50,
    learning_rate = 1,
    random_state 42
)